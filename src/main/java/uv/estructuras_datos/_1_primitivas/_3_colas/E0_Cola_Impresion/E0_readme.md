## Principales operaciones con colas simples

| NÂº | OperaciÃ³n                  | DescripciÃ³n                                                                 | LÃ³gica paso a paso                                                                                                  | Complejidad |
|----|----------------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|-------------|
| 1  | Agregar (Enqueue)          | Inserta un nuevo elemento al final de la cola                               | - Crear nodo <br> - Si estÃ¡ vacÃ­a: frente = fin = nuevo <br> - Si no: fin.siguiente = nuevo; fin = nuevo            | O(1)        |
| 2  | Eliminar (Dequeue)         | Elimina el elemento en el frente de la cola                                 | - Verificar si estÃ¡ vacÃ­a <br> - Guardar frente <br> - frente = frente.siguiente <br> - Si frente = null, fin = null | O(1)        |
| 3  | Eliminar todos             | VacÃ­a completamente la cola                                                 | - frente = null <br> - fin = null                                                                                   | O(1)        |
| 4  | Buscar elemento            | Verifica si un valor estÃ¡ presente en la cola                               | - Recorrer con un puntero desde frente <br> - Comparar nodo por nodo <br> - Retornar true si encuentra, si no false | O(n)        |
| 5  | Mostrar cola               | Imprime todos los elementos en orden desde el frente                        | - Verificar si estÃ¡ vacÃ­a <br> - Recorrer desde frente e imprimir                                                  | O(n)        |
| 6  | Imprimir y vaciar (Procesar) | Simula la impresiÃ³n eliminando uno a uno los elementos                     | - Mientras frente != null: imprimir, luego avanzar frente <br> - fin = null                                         | O(n)        |
| 7  | Fusionar con otra cola     | Une los elementos de dos colas en una nueva sin modificar las originales    | - Crear nueva cola <br> - Recorrer c1 y agregar <br> - Recorrer c2 y agregar                                       | O(n + m)    |
| 8  | Verificar si estÃ¡ vacÃ­a    | Comprueba si la cola no contiene elementos                                  | - Retornar `frente == null`                                                                                        | O(1)        |


#  Fundamentos de la NotaciÃ³n Big-O

## ğŸ¯ Objetivo

Introducir el concepto de notaciÃ³n Big-O para medir la eficiencia de los algoritmos, abordando los diferentes tipos de complejidad temporal, su interpretaciÃ³n y cuÃ¡ndo se presentan.


## ğŸ“˜ Â¿QuÃ© Big-O?

La notaciÃ³n Big-O (O grande) es una forma matemÃ¡tica de describir el **crecimiento del tiempo de ejecuciÃ³n** de un algoritmo en funciÃ³n del tamaÃ±o de su entrada (`n`). No mide el tiempo en segundos, sino la **cantidad de pasos** relativos que realiza el algoritmo.


## ğŸ” Propiedades clave

* **Se enfoca en el peor caso**.
* Ignora constantes y tÃ©rminos menos significativos.
* Nos dice **cÃ³mo escala** un algoritmo cuando crece el tamaÃ±o de los datos.


## ğŸ“Š Tipos comunes de complejidad

| NotaciÃ³n   | Nombre             | DescripciÃ³n                                            | Ejemplo de algoritmo tÃ­pico             |
| ---------- | ------------------ | ------------------------------------------------------ | --------------------------------------- |
| O(1)       | Constante          | El tiempo no depende del tamaÃ±o de la entrada          | Acceso directo a un arreglo             |
| O(log n)   | LogarÃ­tmica        | El tiempo crece lentamente al aumentar la entrada      | BÃºsqueda binaria                        |
| O(n)       | Lineal             | El tiempo crece proporcionalmente al tamaÃ±o de entrada | BÃºsqueda secuencial                     |
| O(n log n) | Lineal logarÃ­tmica | MÃ¡s lento que lineal, pero mejor que cuadrÃ¡tico        | Merge Sort, Quick Sort (promedio)       |
| O(nÂ²)      | CuadrÃ¡tica         | Tiempo crece al cuadrado del tamaÃ±o de entrada         | Burbujas, SelecciÃ³n, InserciÃ³n          |
| O(2â¿)      | Exponencial        | Tiempo se duplica con cada elemento agregado           | Problemas de recursiÃ³n sin optimizaciÃ³n |
| O(n!)      | Factorial          | Todos los Ã³rdenes posibles deben evaluarse             | Algoritmos de permutaciÃ³n, fuerza bruta |


## ğŸ“ Reglas prÃ¡cticas

* Solo se considera el **tÃ©rmino de mayor crecimiento**.

    * Ejemplo: `O(nÂ² + n)` se simplifica a `O(nÂ²)`
* Las constantes **no se consideran**.

    * Ejemplo: `O(3n)` â†’ `O(n)`


## ğŸ§  Â¿Por quÃ© importa?

Comprender la notaciÃ³n Big-O te permite:

* Predecir cuÃ¡n **eficiente** serÃ¡ un algoritmo con datos grandes.
* Comparar **dos soluciones** a un mismo problema.
* Identificar **cuellos de botella** en el diseÃ±o del software.


## ğŸ”§ CÃ³digo ilustrativo para cada tipo de complejidad

En esta secciÃ³n, mostraremos **fragmentos de cÃ³digo en Java** con una breve explicaciÃ³n que ilustre el comportamiento de cada tipo de complejidad.


### ğŸ”¸ **O(1) - Complejidad Constante**

**CÃ³digo asociado:**

```java
int[] arreglo = {10, 20, 30, 40};
System.out.println(arreglo[2]); // acceso directo
```

**ExplicaciÃ³n detallada:**

Esta lÃ­nea de cÃ³digo accede directamente al **tercer elemento** de un arreglo (Ã­ndice 2, ya que en Java los Ã­ndices empiezan en 0). No importa si el arreglo tiene 4, 100 o 10,000 elementos; el tiempo que se tarda en recuperar el valor `30` **siempre serÃ¡ el mismo**.

Â¿Por quÃ©?
Porque los arreglos en Java estÃ¡n organizados en posiciones de memoria contiguas, y el lenguaje puede calcular la direcciÃ³n del elemento deseado con una simple fÃ³rmula:

```
direcciÃ³n_base + (tamaÃ±o_elemento * Ã­ndice)
```

Esto significa que **no tiene que recorrer el arreglo** para encontrar ese elemento. Por eso se dice que el acceso es **constante**, o sea, de **complejidad O(1)**.

ğŸ“Œ **AnalogÃ­a:** Imagina que tienes un archivero con cajones numerados. Puedes abrir directamente el cajÃ³n nÃºmero 3 sin necesidad de revisar el 1 ni el 2.


### ğŸ”¸ **O(log n) - Complejidad LogarÃ­tmica**

**CÃ³digo asociado:**

```java
public int busquedaBinaria(int[] arr, int clave) {
    int inicio = 0, fin = arr.length - 1;
    while (inicio <= fin) {
        int medio = (inicio + fin) / 2;
        if (arr[medio] == clave) return medio;
        if (clave < arr[medio]) fin = medio - 1;
        else inicio = medio + 1;
    }
    return -1;
}
```

**ExplicaciÃ³n detallada:**

La **bÃºsqueda binaria** solo funciona si el arreglo ya estÃ¡ **ordenado**.

1. Empieza examinando el **elemento del medio** del arreglo.
2. Si ese elemento es el que buscas â†’ listo, lo encontraste.
3. Si el nÃºmero buscado es menor al del medio â†’ descartas la **mitad derecha** del arreglo.
4. Si es mayor â†’ descartas la **mitad izquierda**.
5. Repites este proceso con la mitad restante.

Esto se repite hasta encontrar el nÃºmero o hasta que se agoten las posibilidades.
En cada paso, el tamaÃ±o del problema se reduce a la mitad.

Por eso la complejidad es `O(log n)`.

ğŸ“Œ **AnalogÃ­a:** Es como buscar una palabra en un diccionario: abres al medio, decides si estÃ¡ antes o despuÃ©s, y sigues partiendo en mitades.


### ğŸ”¸ **O(n) - Complejidad Lineal**

**CÃ³digo asociado:**

```java
for (int i = 0; i < arreglo.length; i++) {
    System.out.println(arreglo[i]);
}
```

**ExplicaciÃ³n detallada:**

Este es un **recorrido completo** de un arreglo. La variable `i` empieza en 0 y va aumentando hasta llegar al Ãºltimo Ã­ndice.

En cada paso se imprime el valor del elemento en la posiciÃ³n `i`.
Si el arreglo tiene 10 elementos, habrÃ¡ 10 impresiones. Si tiene 1,000, habrÃ¡ 1,000. Por eso el tiempo crece **en proporciÃ³n directa al tamaÃ±o de la entrada**.

Se le llama complejidad lineal `O(n)` porque el **nÃºmero de pasos aumenta linealmente** con la cantidad de elementos que hay.

ğŸ“Œ **AnalogÃ­a:** Es como leer una lista de nombres en voz alta: lees uno por uno, sin saltarte ninguno.


### ğŸ”¸ **O(n log n) - Complejidad Lineal LogarÃ­tmica**

**CÃ³digo asociado:**

```java
public void mergeSort(int[] arr, int izquierda, int derecha) {
    if (izquierda < derecha) {
        int medio = (izquierda + derecha) / 2;
        mergeSort(arr, izquierda, medio);
        mergeSort(arr, medio + 1, derecha);
        merge(arr, izquierda, medio, derecha);
    }
}
```

**ExplicaciÃ³n detallada:**

Este es un ejemplo de **Merge Sort**, un algoritmo de ordenamiento muy eficiente.

Funciona asÃ­:

1. **Divide** el arreglo a la mitad (recursivamente), hasta que cada subarreglo tenga solo un elemento.
2. Luego **combina (merge)** esos subarreglos, **ordenando** en el proceso.

* Cada divisiÃ³n reduce el problema a la mitad â†’ `log n` pasos (como en bÃºsqueda binaria).
* Cada nivel de divisiÃ³n requiere recorrer todo el arreglo para mezclar los subarreglos â†’ `n` pasos.

Por eso la complejidad total es `O(n log n)`.

ğŸ“Œ **AnalogÃ­a:** Imagina que separas un mazo de cartas en montones de una carta y luego las unes ordenadamente. Lo haces en varios niveles.


### ğŸ”¸ **O(nÂ²) - Complejidad CuadrÃ¡tica**

**CÃ³digo asociado:**

```java
for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
        System.out.print("* ");
    }
    System.out.println();
}
```

**ExplicaciÃ³n detallada:**

Este fragmento tiene **dos bucles anidados**, lo que significa que por cada valor de `i`, se ejecuta completamente el bucle interno `j`.

Si `n = 5`, el bucle interno se ejecutarÃ¡ 5 veces por cada una de las 5 iteraciones del bucle externo â†’ 25 pasos.

El nÃºmero total de operaciones serÃ¡:

```
n * n = nÂ²
```

Por eso su complejidad es `O(nÂ²)`.

ğŸ“Œ **AnalogÃ­a:** Es como tener que revisar todos los asientos en una sala de cine para encontrar una pareja: revisas cada fila, y dentro de cada fila, cada asiento.


### ğŸ”¸ **O(2â¿) - Complejidad Exponencial**

**CÃ³digo asociado:**

```java
public int fibonacci(int n) {
    if (n <= 1) return n;
    return fibonacci(n - 1) + fibonacci(n - 2);
}
```

**ExplicaciÃ³n detallada:**

Esta es una **implementaciÃ³n recursiva ingenua** de la serie de Fibonacci.

Cada llamada a la funciÃ³n **genera dos nuevas llamadas**, y esas a su vez, dos mÃ¡sâ€¦
Esto forma un **Ã¡rbol de llamadas** que crece exponencialmente.

Por ejemplo, para calcular `fibonacci(5)` se hacen 15 llamadas. Para `fibonacci(10)`, se hacen 177.
El nÃºmero de operaciones **se duplica** con cada incremento en `n`.

Por eso se dice que su complejidad es `O(2â¿)`.

ğŸ“Œ **AnalogÃ­a:** Es como si cada persona que preguntas te dijera que preguntes a dos mÃ¡s. Pronto tendrÃ­as cientos de personas involucradas.


### ğŸ”¸ **O(n!) - Complejidad Factorial**

**CÃ³digo asociado:**

```java
public void permutar(String prefijo, String resto) {
    if (resto.length() == 0) System.out.println(prefijo);
    for (int i = 0; i < resto.length(); i++) {
        permutar(prefijo + resto.charAt(i), resto.substring(0, i) + resto.substring(i + 1));
    }
}
```

**ExplicaciÃ³n detallada:**

Este algoritmo genera **todas las permutaciones posibles** de un conjunto de caracteres.

Si tienes:

* 2 letras â†’ 2 permutaciones
* 3 letras â†’ 6 permutaciones
* 4 letras â†’ 24 permutaciones

Esto ocurre porque para `n` elementos hay `n!` formas de ordenarlos.

Cada vez que agregas un nuevo carÃ¡cter, **la cantidad de combinaciones se multiplica**.
AsÃ­, el tiempo de ejecuciÃ³n **crece mÃ¡s rÃ¡pido que en cualquier otro tipo de complejidad**.

ğŸ“Œ **AnalogÃ­a:** Es como intentar probar todas las combinaciones posibles de contraseÃ±as. Inviable si hay mÃ¡s de unos pocos elementos.


>Hasta aqui seguramente estaras pensando que las notaciones que involucran el uso de logaritmos no te quedan del todo claras.
> Te invito a que revisemos los logaritmos y su relacion con la notacion Big-O y el codigo.


## ğŸ“š Â¿QuÃ© es un logaritmo y por quÃ© aparece en la notaciÃ³n Big-O?

### ğŸ¯ Objetivo didÃ¡ctico

Que el estudiante **entienda intuitivamente**:

* QuÃ© significa `log n` en algoritmos.
* Por quÃ© aparece en la notaciÃ³n Big-O.
* CÃ³mo se comporta `O(log n)` comparado con `O(n)` o `O(nÂ²)`.
* QuÃ© patrones de cÃ³digo generan esta complejidad.


## ğŸ§  Â¿QuÃ© es un logaritmo?

Un **logaritmo** es el **nÃºmero de veces que debes dividir entre 2 (o entre otro nÃºmero base)** hasta llegar a 1.

Por ejemplo:

| NÃºmero (n) | logâ‚‚(n) |
| ---------- | ------- |
| 1          | 0       |
| 2          | 1       |
| 4          | 2       |
| 8          | 3       |
| 16         | 4       |
| 32         | 5       |
| 64         | 6       |

**Â¿QuÃ© significa `logâ‚‚(64) = 6`?**
â†’ Que puedes dividir 64 entre 2 exactamente **6 veces** hasta obtener 1.


## ğŸ“Š ComparaciÃ³n de crecimientos

| n (tamaÃ±o) | O(1) | O(log n) | O(n) | O(n log n) | O(nÂ²)     |
| ---------- | ---- | -------- | ---- | ---------- | --------- |
| 10         | 1    | 3.3      | 10   | 33         | 100       |
| 100        | 1    | 6.6      | 100  | 660        | 10,000    |
| 1,000      | 1    | 9.9      | 1000 | 9,900      | 1,000,000 |

ğŸ“Œ **ConclusiÃ³n:** El logaritmo crece **lentamente**, incluso con valores muy grandes de `n`.


## ğŸ” Â¿CÃ³mo se relaciona con el cÃ³digo?

Cuando en un algoritmo **se reduce el tamaÃ±o del problema a la mitad en cada paso**, su complejidad suele ser `O(log n)`.

---

## ğŸ“¦ Ejemplo: BÃºsqueda Binaria

```java
public int buscar(int[] arreglo, int valor) {
    int inicio = 0, fin = arreglo.length - 1;
    while (inicio <= fin) {
        int medio = (inicio + fin) / 2;
        if (arreglo[medio] == valor) return medio;
        if (valor < arreglo[medio]) fin = medio - 1;
        else inicio = medio + 1;
    }
    return -1;
}
```

### ExplicaciÃ³n paso a paso:

1. El arreglo estÃ¡ ordenado.
2. Empiezo por el **centro**.
3. Cada vez que **descarto la mitad del arreglo**.
4. Esto ocurre una y otra vez, hasta que el tamaÃ±o del problema es 1.

---

## ğŸ§  Â¿Por quÃ© `O(log n)`?

Porque el nÃºmero de veces que se puede dividir un arreglo entre 2 antes de que quede 1 elemento es precisamente `logâ‚‚(n)`.

---

### ğŸ” VisualizaciÃ³n del comportamiento

Supongamos que tengo un arreglo de 64 elementos:

1. Divido en 2 â†’ 32
2. Divido en 2 â†’ 16
3. Divido en 2 â†’ 8
4. Divido en 2 â†’ 4
5. Divido en 2 â†’ 2
6. Divido en 2 â†’ 1

â†’ Hice 6 divisiones â‡’ `logâ‚‚(64) = 6` â†’ el algoritmo hizo 6 pasos.


### ğŸ“Œ AnalogÃ­a simple

ğŸ“– **Buscar una palabra en el diccionario:**

* No revisas una por una desde la A hasta la Z.
* Abres al centro, ves si estÃ¡s antes o despuÃ©s de la palabra buscada, y te mueves.
* Con cada paso, descartas **la mitad de las pÃ¡ginas**.
* Esa es la esencia de un algoritmo logarÃ­tmico.



> â€œCuando un algoritmo descarta la mitad del problema en cada paso, su tiempo de ejecuciÃ³n crece **como logaritmo**, no como una lÃ­nea recta. Eso lo hace **rÃ¡pido incluso con entradas muy grandes**.â€


## ğŸ§ª Â¿QuÃ© otros algoritmos son `O(log n)`?

* **BÃºsqueda binaria** en arreglos ordenados.
* Operaciones en **Ã¡rboles balanceados** (AVL, Red-Black Tree).
* **Heap**: inserciÃ³n y extracciÃ³n en colas de prioridad.


Por supuesto. A continuaciÃ³n te presento una **explicaciÃ³n detallada y completamente desarrollada** de la complejidad **O(n log n)**, siguiendo el mismo estilo pedagÃ³gico e ilustrativo que en los casos anteriores. Este tipo de complejidad es especialmente importante en algoritmos de ordenamiento eficientes.

---

# ğŸ“ SecciÃ³n: Complejidad **O(n log n)**

---

## ğŸ¯ Â¿QuÃ© significa `O(n log n)`?

Esta notaciÃ³n indica que el tiempo de ejecuciÃ³n de un algoritmo **crece mÃ¡s rÃ¡pido que linealmente**, pero **no tanto como cuadrÃ¡ticamente**.

* El **`log n`** proviene de **dividir el problema** en partes (como en `O(log n)`).
* El **`n`** representa que **cada elemento se debe procesar** en cada una de esas divisiones.

---

## ğŸ”„ Â¿QuÃ© tipo de algoritmos generan esta complejidad?

Los algoritmos **"divide y vencerÃ¡s"** (Divide and Conquer) mÃ¡s eficientes suelen tener esta complejidad.

Ejemplos:

* **Merge Sort**
* **Quick Sort** (en el caso promedio)
* **Heap Sort**
* Algunos algoritmos de conteo y agrupamiento (como algunos radix/counting en su forma mejorada)

---

## ğŸ“¦ CÃ³digo ilustrativo: `Merge Sort`

Este es el cÃ³digo de referencia que ilustra este tipo de complejidad:

```java
public void mergeSort(int[] arr, int izquierda, int derecha) {
    if (izquierda < derecha) {
        int medio = (izquierda + derecha) / 2;
        mergeSort(arr, izquierda, medio);
        mergeSort(arr, medio + 1, derecha);
        merge(arr, izquierda, medio, derecha);
    }
}
```

## ğŸ§  ExplicaciÃ³n detallada paso a paso

### ğŸ”¹ Fase 1: DivisiÃ³n (log n pasos)

* El arreglo se **divide a la mitad** de forma recursiva.
* Se repite hasta que cada subarreglo tiene un solo elemento.

Ejemplo para un arreglo de 8 elementos:

```
Original:         [A B C D E F G H]
DivisiÃ³n 1:       [A B C D]   [E F G H]
DivisiÃ³n 2:   [A B] [C D]     [E F] [G H]
DivisiÃ³n 3: [A] [B] [C] [D]   [E] [F] [G] [H]
```

ğŸ“Œ **NÃºmero de niveles de divisiÃ³n**: `logâ‚‚(n)`
Para `n = 8`, hay 3 niveles de divisiÃ³n â†’ `logâ‚‚(8) = 3`

---

### ğŸ”¹ Fase 2: CombinaciÃ³n (n operaciones por nivel)

Una vez que tenemos subarreglos de 1 elemento, empezamos a **combinarlos (merge)**:

* Se comparan y ordenan pares de elementos.
* Cada nivel de la combinaciÃ³n **procesa todos los elementos** al menos una vez.

Ejemplo para 8 elementos:

```
[A] + [B] â†’ [A B]
[C] + [D] â†’ [C D]
...
[A B] + [C D] â†’ [A B C D]
...
â†’ [A B C D E F G H]
```

ğŸ“Œ **En cada nivel**, los `n` elementos se comparan y mueven â†’ `n` operaciones.

---

### ğŸ” Â¿Por quÃ© `O(n log n)`?

* Hay **log n niveles** de recursiÃ³n por la divisiÃ³n.
* En cada nivel, se hace un trabajo **proporcional a n** para fusionar.

Por lo tanto:

```text
Trabajo total = (n elementos por nivel) Ã— (log n niveles) = O(n log n)
```


## ğŸ”¬ ComparaciÃ³n con otras complejidades

| TamaÃ±o (`n`) | O(n)  | O(n log n) | O(nÂ²)     |
| ------------ | ----- | ---------- | --------- |
| 10           | 10    | 33         | 100       |
| 100          | 100   | 664        | 10,000    |
| 1,000        | 1,000 | \~10,000   | 1,000,000 |

ğŸ“Œ **O(n log n)** es ideal para algoritmos de ordenamiento de propÃ³sito general.
ğŸ“Œ Mucho mÃ¡s eficiente que `O(nÂ²)` en datos grandes.



