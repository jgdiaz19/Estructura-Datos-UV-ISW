## Principales operaciones con colas simples

| N¬∫ | Operaci√≥n                  | Descripci√≥n                                                                 | L√≥gica paso a paso                                                                                                  | Complejidad |
|----|----------------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|-------------|
| 1  | Agregar (Enqueue)          | Inserta un nuevo elemento al final de la cola                               | - Crear nodo <br> - Si est√° vac√≠a: frente = fin = nuevo <br> - Si no: fin.siguiente = nuevo; fin = nuevo            | O(1)        |
| 2  | Eliminar (Dequeue)         | Elimina el elemento en el frente de la cola                                 | - Verificar si est√° vac√≠a <br> - Guardar frente <br> - frente = frente.siguiente <br> - Si frente = null, fin = null | O(1)        |
| 3  | Eliminar todos             | Vac√≠a completamente la cola                                                 | - frente = null <br> - fin = null                                                                                   | O(1)        |
| 4  | Buscar elemento            | Verifica si un valor est√° presente en la cola                               | - Recorrer con un puntero desde frente <br> - Comparar nodo por nodo <br> - Retornar true si encuentra, si no false | O(n)        |
| 5  | Mostrar cola               | Imprime todos los elementos en orden desde el frente                        | - Verificar si est√° vac√≠a <br> - Recorrer desde frente e imprimir                                                  | O(n)        |
| 6  | Imprimir y vaciar (Procesar) | Simula la impresi√≥n eliminando uno a uno los elementos                     | - Mientras frente != null: imprimir, luego avanzar frente <br> - fin = null                                         | O(n)        |
| 7  | Fusionar con otra cola     | Une los elementos de dos colas en una nueva sin modificar las originales    | - Crear nueva cola <br> - Recorrer c1 y agregar <br> - Recorrer c2 y agregar                                       | O(n + m)    |
| 8  | Verificar si est√° vac√≠a    | Comprueba si la cola no contiene elementos                                  | - Retornar `frente == null`                                                                                        | O(1)        |


#  Fundamentos de la Notaci√≥n Big-O

## üéØ Objetivo

Introducir el concepto de notaci√≥n Big-O para medir la eficiencia de los algoritmos, abordando los diferentes tipos de complejidad temporal, su interpretaci√≥n y cu√°ndo se presentan.


## üìò ¬øQu√© Big-O?

La notaci√≥n Big-O (O grande) es una forma matem√°tica de describir el **crecimiento del tiempo de ejecuci√≥n** de un algoritmo en funci√≥n del tama√±o de su entrada (`n`). No mide el tiempo en segundos, sino la **cantidad de pasos** relativos que realiza el algoritmo.


## üîç Propiedades clave

* **Se enfoca en el peor caso**.
* Ignora constantes y t√©rminos menos significativos.
* Nos dice **c√≥mo escala** un algoritmo cuando crece el tama√±o de los datos.


## üìä Tipos comunes de complejidad

| Notaci√≥n   | Nombre             | Descripci√≥n                                            | Ejemplo de algoritmo t√≠pico             |
| ---------- | ------------------ | ------------------------------------------------------ | --------------------------------------- |
| O(1)       | Constante          | El tiempo no depende del tama√±o de la entrada          | Acceso directo a un arreglo             |
| O(log n)   | Logar√≠tmica        | El tiempo crece lentamente al aumentar la entrada      | B√∫squeda binaria                        |
| O(n)       | Lineal             | El tiempo crece proporcionalmente al tama√±o de entrada | B√∫squeda secuencial                     |
| O(n log n) | Lineal logar√≠tmica | M√°s lento que lineal, pero mejor que cuadr√°tico        | Merge Sort, Quick Sort (promedio)       |
| O(n¬≤)      | Cuadr√°tica         | Tiempo crece al cuadrado del tama√±o de entrada         | Burbujas, Selecci√≥n, Inserci√≥n          |
| O(2‚Åø)      | Exponencial        | Tiempo se duplica con cada elemento agregado           | Problemas de recursi√≥n sin optimizaci√≥n |
| O(n!)      | Factorial          | Todos los √≥rdenes posibles deben evaluarse             | Algoritmos de permutaci√≥n, fuerza bruta |


## üéì Reglas pr√°cticas

* Solo se considera el **t√©rmino de mayor crecimiento**.

    * Ejemplo: `O(n¬≤ + n)` se simplifica a `O(n¬≤)`
* Las constantes **no se consideran**.

    * Ejemplo: `O(3n)` ‚Üí `O(n)`


## üß† ¬øPor qu√© importa?

Comprender la notaci√≥n Big-O te permite:

* Predecir cu√°n **eficiente** ser√° un algoritmo con datos grandes.
* Comparar **dos soluciones** a un mismo problema.
* Identificar **cuellos de botella** en el dise√±o del software.


## üîß C√≥digo ilustrativo para cada tipo de complejidad

En esta secci√≥n, mostraremos **fragmentos de c√≥digo en Java** con una breve explicaci√≥n que ilustre el comportamiento de cada tipo de complejidad.


### üî∏ **O(1) - Complejidad Constante**

**C√≥digo asociado:**

```java
int[] arreglo = {10, 20, 30, 40};
System.out.println(arreglo[2]); // acceso directo
```

**Explicaci√≥n detallada:**

Esta l√≠nea de c√≥digo accede directamente al **tercer elemento** de un arreglo (√≠ndice 2, ya que en Java los √≠ndices empiezan en 0). No importa si el arreglo tiene 4, 100 o 10,000 elementos; el tiempo que se tarda en recuperar el valor `30` **siempre ser√° el mismo**.

¬øPor qu√©?
Porque los arreglos en Java est√°n organizados en posiciones de memoria contiguas, y el lenguaje puede calcular la direcci√≥n del elemento deseado con una simple f√≥rmula:

```
direcci√≥n_base + (tama√±o_elemento * √≠ndice)
```

Esto significa que **no tiene que recorrer el arreglo** para encontrar ese elemento. Por eso se dice que el acceso es **constante**, o sea, de **complejidad O(1)**.

üìå **Analog√≠a:** Imagina que tienes un archivero con cajones numerados. Puedes abrir directamente el caj√≥n n√∫mero 3 sin necesidad de revisar el 1 ni el 2.


### üî∏ **O(log n) - Complejidad Logar√≠tmica**

**C√≥digo asociado:**

```java
public int busquedaBinaria(int[] arr, int clave) {
    int inicio = 0, fin = arr.length - 1;
    while (inicio <= fin) {
        int medio = (inicio + fin) / 2;
        if (arr[medio] == clave) return medio;
        if (clave < arr[medio]) fin = medio - 1;
        else inicio = medio + 1;
    }
    return -1;
}
```

**Explicaci√≥n detallada:**

La **b√∫squeda binaria** solo funciona si el arreglo ya est√° **ordenado**.

1. Empieza examinando el **elemento del medio** del arreglo.
2. Si ese elemento es el que buscas ‚Üí listo, lo encontraste.
3. Si el n√∫mero buscado es menor al del medio ‚Üí descartas la **mitad derecha** del arreglo.
4. Si es mayor ‚Üí descartas la **mitad izquierda**.
5. Repites este proceso con la mitad restante.

Esto se repite hasta encontrar el n√∫mero o hasta que se agoten las posibilidades.
En cada paso, el tama√±o del problema se reduce a la mitad.

Por eso la complejidad es `O(log n)`.

üìå **Analog√≠a:** Es como buscar una palabra en un diccionario: abres al medio, decides si est√° antes o despu√©s, y sigues partiendo en mitades.


### üî∏ **O(n) - Complejidad Lineal**

**C√≥digo asociado:**

```java
for (int i = 0; i < arreglo.length; i++) {
    System.out.println(arreglo[i]);
}
```

**Explicaci√≥n detallada:**

Este es un **recorrido completo** de un arreglo. La variable `i` empieza en 0 y va aumentando hasta llegar al √∫ltimo √≠ndice.

En cada paso se imprime el valor del elemento en la posici√≥n `i`.
Si el arreglo tiene 10 elementos, habr√° 10 impresiones. Si tiene 1,000, habr√° 1,000. Por eso el tiempo crece **en proporci√≥n directa al tama√±o de la entrada**.

Se le llama complejidad lineal `O(n)` porque el **n√∫mero de pasos aumenta linealmente** con la cantidad de elementos que hay.

üìå **Analog√≠a:** Es como leer una lista de nombres en voz alta: lees uno por uno, sin saltarte ninguno.


### üî∏ **O(n log n) - Complejidad Lineal Logar√≠tmica**

**C√≥digo asociado:**

```java
public void mergeSort(int[] arr, int izquierda, int derecha) {
    if (izquierda < derecha) {
        int medio = (izquierda + derecha) / 2;
        mergeSort(arr, izquierda, medio);
        mergeSort(arr, medio + 1, derecha);
        merge(arr, izquierda, medio, derecha);
    }
}
```

**Explicaci√≥n detallada:**

Este es un ejemplo de **Merge Sort**, un algoritmo de ordenamiento muy eficiente.

Funciona as√≠:

1. **Divide** el arreglo a la mitad (recursivamente), hasta que cada subarreglo tenga solo un elemento.
2. Luego **combina (merge)** esos subarreglos, **ordenando** en el proceso.

* Cada divisi√≥n reduce el problema a la mitad ‚Üí `log n` pasos (como en b√∫squeda binaria).
* Cada nivel de divisi√≥n requiere recorrer todo el arreglo para mezclar los subarreglos ‚Üí `n` pasos.

Por eso la complejidad total es `O(n log n)`.

üìå **Analog√≠a:** Imagina que separas un mazo de cartas en montones de una carta y luego las unes ordenadamente. Lo haces en varios niveles.


### üî∏ **O(n¬≤) - Complejidad Cuadr√°tica**

**C√≥digo asociado:**

```java
for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
        System.out.print("* ");
    }
    System.out.println();
}
```

**Explicaci√≥n detallada:**

Este fragmento tiene **dos bucles anidados**, lo que significa que por cada valor de `i`, se ejecuta completamente el bucle interno `j`.

Si `n = 5`, el bucle interno se ejecutar√° 5 veces por cada una de las 5 iteraciones del bucle externo ‚Üí 25 pasos.

El n√∫mero total de operaciones ser√°:

```
n * n = n¬≤
```

Por eso su complejidad es `O(n¬≤)`.

üìå **Analog√≠a:** Es como tener que revisar todos los asientos en una sala de cine para encontrar una pareja: revisas cada fila, y dentro de cada fila, cada asiento.


### üî∏ **O(2‚Åø) - Complejidad Exponencial**

**C√≥digo asociado:**

```java
public int fibonacci(int n) {
    if (n <= 1) return n;
    return fibonacci(n - 1) + fibonacci(n - 2);
}
```

**Explicaci√≥n detallada:**

Esta es una **implementaci√≥n recursiva ingenua** de la serie de Fibonacci.

Cada llamada a la funci√≥n **genera dos nuevas llamadas**, y esas a su vez, dos m√°s‚Ä¶
Esto forma un **√°rbol de llamadas** que crece exponencialmente.

Por ejemplo, para calcular `fibonacci(5)` se hacen 15 llamadas. Para `fibonacci(10)`, se hacen 177.
El n√∫mero de operaciones **se duplica** con cada incremento en `n`.

Por eso se dice que su complejidad es `O(2‚Åø)`.

üìå **Analog√≠a:** Es como si cada persona que preguntas te dijera que preguntes a dos m√°s. Pronto tendr√≠as cientos de personas involucradas.


### üî∏ **O(n!) - Complejidad Factorial**

**C√≥digo asociado:**

```java
public void permutar(String prefijo, String resto) {
    if (resto.length() == 0) System.out.println(prefijo);
    for (int i = 0; i < resto.length(); i++) {
        permutar(prefijo + resto.charAt(i), resto.substring(0, i) + resto.substring(i + 1));
    }
}
```

**Explicaci√≥n detallada:**

Este algoritmo genera **todas las permutaciones posibles** de un conjunto de caracteres.

Si tienes:

* 2 letras ‚Üí 2 permutaciones
* 3 letras ‚Üí 6 permutaciones
* 4 letras ‚Üí 24 permutaciones

Esto ocurre porque para `n` elementos hay `n!` formas de ordenarlos.

Cada vez que agregas un nuevo car√°cter, **la cantidad de combinaciones se multiplica**.
As√≠, el tiempo de ejecuci√≥n **crece m√°s r√°pido que en cualquier otro tipo de complejidad**.

üìå **Analog√≠a:** Es como intentar probar todas las combinaciones posibles de contrase√±as. Inviable si hay m√°s de unos pocos elementos.


>Hasta aqui seguramente estaras pensando que las notaciones que involucran el uso de logaritmos no te quedan del todo claras.
> Te invito a que revisemos los logaritmos y su relacion con la notacion Big-O y el codigo.


## üìö ¬øQu√© es un logaritmo y por qu√© aparece en la notaci√≥n Big-O?

### üéØ Objetivo did√°ctico

Que el estudiante **entienda intuitivamente**:

* Qu√© significa `log n` en algoritmos.
* Por qu√© aparece en la notaci√≥n Big-O.
* C√≥mo se comporta `O(log n)` comparado con `O(n)` o `O(n¬≤)`.
* Qu√© patrones de c√≥digo generan esta complejidad.


## üß† ¬øQu√© es un logaritmo?

Un **logaritmo** es el **n√∫mero de veces que debes dividir entre 2 (o entre otro n√∫mero base)** hasta llegar a 1.

Por ejemplo:

| N√∫mero (n) | log‚ÇÇ(n) |
| ---------- | ------- |
| 1          | 0       |
| 2          | 1       |
| 4          | 2       |
| 8          | 3       |
| 16         | 4       |
| 32         | 5       |
| 64         | 6       |

**¬øQu√© significa `log‚ÇÇ(64) = 6`?**
‚Üí Que puedes dividir 64 entre 2 exactamente **6 veces** hasta obtener 1.


## üìä Comparaci√≥n de crecimientos

| n (tama√±o) | O(1) | O(log n) | O(n) | O(n log n) | O(n¬≤)     |
| ---------- | ---- | -------- | ---- | ---------- | --------- |
| 10         | 1    | 3.3      | 10   | 33         | 100       |
| 100        | 1    | 6.6      | 100  | 660        | 10,000    |
| 1,000      | 1    | 9.9      | 1000 | 9,900      | 1,000,000 |

üìå **Conclusi√≥n:** El logaritmo crece **lentamente**, incluso con valores muy grandes de `n`.


## üîç ¬øC√≥mo se relaciona con el c√≥digo?

Cuando en un algoritmo **se reduce el tama√±o del problema a la mitad en cada paso**, su complejidad suele ser `O(log n)`.

---

## üì¶ Ejemplo: B√∫squeda Binaria

```java
public int buscar(int[] arreglo, int valor) {
    int inicio = 0, fin = arreglo.length - 1;
    while (inicio <= fin) {
        int medio = (inicio + fin) / 2;
        if (arreglo[medio] == valor) return medio;
        if (valor < arreglo[medio]) fin = medio - 1;
        else inicio = medio + 1;
    }
    return -1;
}
```

### Explicaci√≥n paso a paso:

1. El arreglo est√° ordenado.
2. Empiezo por el **centro**.
3. Cada vez que **descarto la mitad del arreglo**.
4. Esto ocurre una y otra vez, hasta que el tama√±o del problema es 1.

---

## üß† ¬øPor qu√© `O(log n)`?

Porque el n√∫mero de veces que se puede dividir un arreglo entre 2 antes de que quede 1 elemento es precisamente `log‚ÇÇ(n)`.

---

### üîÅ Visualizaci√≥n del comportamiento

Supongamos que tengo un arreglo de 64 elementos:

1. Divido en 2 ‚Üí 32
2. Divido en 2 ‚Üí 16
3. Divido en 2 ‚Üí 8
4. Divido en 2 ‚Üí 4
5. Divido en 2 ‚Üí 2
6. Divido en 2 ‚Üí 1

‚Üí Hice 6 divisiones ‚áí `log‚ÇÇ(64) = 6` ‚Üí el algoritmo hizo 6 pasos.


### üìå Analog√≠a simple

üìñ **Buscar una palabra en el diccionario:**

* No revisas una por una desde la A hasta la Z.
* Abres al centro, ves si est√°s antes o despu√©s de la palabra buscada, y te mueves.
* Con cada paso, descartas **la mitad de las p√°ginas**.
* Esa es la esencia de un algoritmo logar√≠tmico.



> ‚ÄúCuando un algoritmo descarta la mitad del problema en cada paso, su tiempo de ejecuci√≥n crece **como logaritmo**, no como una l√≠nea recta. Eso lo hace **r√°pido incluso con entradas muy grandes**.‚Äù


## üß™ ¬øQu√© otros algoritmos son `O(log n)`?

* **B√∫squeda binaria** en arreglos ordenados.
* Operaciones en **√°rboles balanceados** (AVL, Red-Black Tree).
* **Heap**: inserci√≥n y extracci√≥n en colas de prioridad.


Por supuesto. A continuaci√≥n te presento una **explicaci√≥n detallada y completamente desarrollada** de la complejidad **O(n log n)**, siguiendo el mismo estilo pedag√≥gico e ilustrativo que en los casos anteriores. Este tipo de complejidad es especialmente importante en algoritmos de ordenamiento eficientes.

---

# üìê Secci√≥n: Complejidad **O(n log n)**

---

## üéØ ¬øQu√© significa `O(n log n)`?

Esta notaci√≥n indica que el tiempo de ejecuci√≥n de un algoritmo **crece m√°s r√°pido que linealmente**, pero **no tanto como cuadr√°ticamente**.

* El **`log n`** proviene de **dividir el problema** en partes (como en `O(log n)`).
* El **`n`** representa que **cada elemento se debe procesar** en cada una de esas divisiones.

---

## üîÑ ¬øQu√© tipo de algoritmos generan esta complejidad?

Los algoritmos **"divide y vencer√°s"** (Divide and Conquer) m√°s eficientes suelen tener esta complejidad.

Ejemplos:

* **Merge Sort**
* **Quick Sort** (en el caso promedio)
* **Heap Sort**
* Algunos algoritmos de conteo y agrupamiento (como algunos radix/counting en su forma mejorada)

---

## üì¶ C√≥digo ilustrativo: `Merge Sort`

Este es el c√≥digo de referencia que ilustra este tipo de complejidad:

```java
public void mergeSort(int[] arr, int izquierda, int derecha) {
    if (izquierda < derecha) {
        int medio = (izquierda + derecha) / 2;
        mergeSort(arr, izquierda, medio);
        mergeSort(arr, medio + 1, derecha);
        merge(arr, izquierda, medio, derecha);
    }
}
```

## üß† Explicaci√≥n detallada paso a paso

### üîπ Fase 1: Divisi√≥n (log n pasos)

* El arreglo se **divide a la mitad** de forma recursiva.
* Se repite hasta que cada subarreglo tiene un solo elemento.

Ejemplo para un arreglo de 8 elementos:

```
Original:         [A B C D E F G H]
Divisi√≥n 1:       [A B C D]   [E F G H]
Divisi√≥n 2:   [A B] [C D]     [E F] [G H]
Divisi√≥n 3: [A] [B] [C] [D]   [E] [F] [G] [H]
```

üìå **N√∫mero de niveles de divisi√≥n**: `log‚ÇÇ(n)`
Para `n = 8`, hay 3 niveles de divisi√≥n ‚Üí `log‚ÇÇ(8) = 3`

---

### üîπ Fase 2: Combinaci√≥n (n operaciones por nivel)

Una vez que tenemos subarreglos de 1 elemento, empezamos a **combinarlos (merge)**:

* Se comparan y ordenan pares de elementos.
* Cada nivel de la combinaci√≥n **procesa todos los elementos** al menos una vez.

Ejemplo para 8 elementos:

```
[A] + [B] ‚Üí [A B]
[C] + [D] ‚Üí [C D]
...
[A B] + [C D] ‚Üí [A B C D]
...
‚Üí [A B C D E F G H]
```

üìå **En cada nivel**, los `n` elementos se comparan y mueven ‚Üí `n` operaciones.

---

### üîÅ ¬øPor qu√© `O(n log n)`?

* Hay **log n niveles** de recursi√≥n por la divisi√≥n.
* En cada nivel, se hace un trabajo **proporcional a n** para fusionar.

Por lo tanto:

```text
Trabajo total = (n elementos por nivel) √ó (log n niveles) = O(n log n)
```


## üî¨ Comparaci√≥n con otras complejidades

| Tama√±o (`n`) | O(n)  | O(n log n) | O(n¬≤)     |
| ------------ | ----- | ---------- | --------- |
| 10           | 10    | 33         | 100       |
| 100          | 100   | 664        | 10,000    |
| 1,000        | 1,000 | \~10,000   | 1,000,000 |

üìå **O(n log n)** es ideal para algoritmos de ordenamiento de prop√≥sito general.
üìå Mucho m√°s eficiente que `O(n¬≤)` en datos grandes.



